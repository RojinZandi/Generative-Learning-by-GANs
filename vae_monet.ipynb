{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 2019.579725,
      "end_time": "2021-04-24T23:03:44.605646",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-04-24T22:30:05.025921",
      "version": "2.1.0"
    },
    "colab": {
      "name": "vae-monet.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUywADmW9c2Y"
      },
      "source": [
        "VAE for Monet Painting\n",
        "\n",
        "refer to section 4.1 in the report please"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "execution": {
          "iopub.execute_input": "2021-04-24T22:30:16.794860Z",
          "iopub.status.busy": "2021-04-24T22:30:16.793662Z",
          "iopub.status.idle": "2021-04-24T22:30:18.616283Z",
          "shell.execute_reply": "2021-04-24T22:30:18.615160Z"
        },
        "papermill": {
          "duration": 1.863374,
          "end_time": "2021-04-24T22:30:18.616406",
          "exception": false,
          "start_time": "2021-04-24T22:30:16.753032",
          "status": "completed"
        },
        "tags": [],
        "id": "H7tD68qF8p7M"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.utils.data as data\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-24T22:30:18.665287Z",
          "iopub.status.busy": "2021-04-24T22:30:18.664413Z",
          "iopub.status.idle": "2021-04-24T22:30:18.667612Z",
          "shell.execute_reply": "2021-04-24T22:30:18.667105Z"
        },
        "papermill": {
          "duration": 0.03008,
          "end_time": "2021-04-24T22:30:18.667743",
          "exception": false,
          "start_time": "2021-04-24T22:30:18.637663",
          "status": "completed"
        },
        "tags": [],
        "id": "nWejjEgW8p7N"
      },
      "source": [
        "# 2-d latent space, parameter count in same order of magnitude\n",
        "# as in the original VAE paper (VAE paper has about 3x as many)\n",
        "latent_dims = 2\n",
        "num_epochs = 20 # Originally at 100\n",
        "batch_size = 128\n",
        "capacity = 64\n",
        "learning_rate = 1e-4\n",
        "variational_beta = 1\n",
        "use_gpu = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-24T22:30:18.726092Z",
          "iopub.status.busy": "2021-04-24T22:30:18.724904Z",
          "iopub.status.idle": "2021-04-24T22:30:21.948986Z",
          "shell.execute_reply": "2021-04-24T22:30:21.948217Z"
        },
        "papermill": {
          "duration": 3.260579,
          "end_time": "2021-04-24T22:30:21.949098",
          "exception": false,
          "start_time": "2021-04-24T22:30:18.688519",
          "status": "completed"
        },
        "tags": [],
        "id": "2SZ1LayL8p7N",
        "outputId": "119bc32a-eb6f-4355-f760-6875307acc2e"
      },
      "source": [
        "# Make the dataset\n",
        "\n",
        "# Configure directory to look appropriate for making dataset later\n",
        "try:\n",
        "  !mkdir ../working/actual_monet_jpg\n",
        "  !cp -R ../input/gan-getting-started/monet_jpg ../working/actual_monet_jpg/monet_jpg_inner\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Define paramaters and transformation for data augmentation\n",
        "BATCH_SIZE = 10\n",
        "DATA_PATH = '../working/actual_monet_jpg'\n",
        "myTransforms = transforms.Compose([\n",
        "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# make loader and data loader\n",
        "train_data = torchvision.datasets.ImageFolder(root=DATA_PATH, transform = myTransforms)\n",
        "train_data = torch.utils.data.ConcatDataset([train_data] * 32)\n",
        "train_data_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6hJ9N_z83zM"
      },
      "source": [
        "# Bulding the VAE\n",
        "\n",
        "first bulding the encoder, decoder, and then the general achitecture of VAE. the next step is training the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-24T22:30:22.031557Z",
          "iopub.status.busy": "2021-04-24T22:30:22.001047Z",
          "iopub.status.idle": "2021-04-24T22:30:27.646553Z",
          "shell.execute_reply": "2021-04-24T22:30:27.647279Z"
        },
        "papermill": {
          "duration": 5.676721,
          "end_time": "2021-04-24T22:30:27.647430",
          "exception": false,
          "start_time": "2021-04-24T22:30:21.970709",
          "status": "completed"
        },
        "tags": [],
        "id": "vBKjo8Y58p7O",
        "outputId": "c38f8ca9-d229-4338-a2dc-478c0e2f3869"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.conv00 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=4, stride=3, padding=0) # out: (c/4) x 85 x 85\n",
        "        self.conv01 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=3, padding=0) # out: (c/2) x 85 x 85\n",
        "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=c, kernel_size=4, stride=2, padding=1) # out: c x 14 x 14 (CHANGE in_channels=1 for monet)\n",
        "        self.conv2 = nn.Conv2d(in_channels=c, out_channels=c*2, kernel_size=4, stride=2, padding=1) # out: c x 7 x 7\n",
        "        self.fc_mu = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "        self.fc_logvar = nn.Linear(in_features=c*2*7*7, out_features=latent_dims)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv00(x)) # for monet\n",
        "        x = F.relu(self.conv01(x)) # for monet\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1) # flatten batch of multi-channel feature maps to a batch of feature vectors\n",
        "        x_mu = self.fc_mu(x)\n",
        "        x_logvar = self.fc_logvar(x)\n",
        "        return x_mu, x_logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        c = capacity\n",
        "        self.fc = nn.Linear(in_features=latent_dims, out_features=c*2*7*7)\n",
        "        self.conv2 = nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv1 = nn.ConvTranspose2d(in_channels=c, out_channels=32, kernel_size=4, stride=2, padding=1) # change to out_channels = 32 for monet\n",
        "        self.conv01 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=3, padding=0)\n",
        "        self.conv00 = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=4, stride=3, padding=0)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.size(0), capacity*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv01(x))\n",
        "        x = torch.sigmoid(self.conv00(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
        "        return x\n",
        "    \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        latent_mu, latent_logvar = self.encoder(x)\n",
        "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
        "        x_recon = self.decoder(latent)\n",
        "        return x_recon, latent_mu, latent_logvar\n",
        "    \n",
        "    def latent_sample(self, mu, logvar):\n",
        "        if self.training:\n",
        "            # the reparameterization trick\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = torch.empty_like(std).normal_()\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # recon_x is the probability of a multivariate Bernoulli distribution p.\n",
        "    # -log(p(x)) is then the pixel-wise binary cross-entropy.\n",
        "    # Averaging or not averaging the binary cross-entropy over all pixels here\n",
        "    # is a subtle detail with big effect on training, since it changes the weight\n",
        "    # we need to pick for the other loss term by several orders of magnitude.\n",
        "    # Not averaging is the direct implementation of the negative log likelihood,\n",
        "    # but averaging makes the weight of the other loss term independent of the image resolution.\n",
        "    size = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n",
        "    #print(size)\n",
        "    recon_loss = F.binary_cross_entropy(recon_x.view(-1, size), x.view(-1, size), reduction='sum') # change 784 to 6291456 for monet\n",
        "    \n",
        "    # KL-divergence between the prior distribution over latent vectors\n",
        "    # (the one we are going to sample from when generating new images)\n",
        "    # and the distribution estimated by the generator for the given image.\n",
        "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    return recon_loss + variational_beta * kldivergence\n",
        "    \n",
        "    \n",
        "vae = VariationalAutoencoder()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "vae = vae.to(device)\n",
        "\n",
        "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
        "print('Number of parameters: %d' % num_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of parameters: 389863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-24T22:30:27.706836Z",
          "iopub.status.busy": "2021-04-24T22:30:27.706161Z",
          "iopub.status.idle": "2021-04-24T23:03:09.409645Z",
          "shell.execute_reply": "2021-04-24T23:03:09.410121Z"
        },
        "papermill": {
          "duration": 1961.740534,
          "end_time": "2021-04-24T23:03:09.410249",
          "exception": false,
          "start_time": "2021-04-24T22:30:27.669715",
          "status": "completed"
        },
        "tags": [],
        "id": "cfBzI3Dq8p7Q",
        "outputId": "202db1f2-9ebf-4387-e29a-a2b9f00fa1c4"
      },
      "source": [
        "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# set to training mode\n",
        "vae.train()\n",
        "\n",
        "train_loss_avg = []\n",
        "\n",
        "print('Training ...')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss_avg.append(0)\n",
        "    num_batches = 0\n",
        "    \n",
        "    for image_batch, _ in train_data_loader:\n",
        "        #print(image_batch.size())\n",
        "        image_batch = image_batch.to(device)\n",
        "        #print(image_batch.size())\n",
        "\n",
        "        # vae reconstruction\n",
        "        image_batch_recon, latent_mu, latent_logvar = vae(image_batch)\n",
        "        # RuntimeError: Given groups=1, weight of size [64, 1, 4, 4], expected input[32, 3, 256, 256] to have 1 channels, but got 3 channels instead\n",
        "        \n",
        "        # reconstruction error\n",
        "        loss = vae_loss(image_batch_recon, image_batch, latent_mu, latent_logvar)\n",
        "        \n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # one step of the optmizer (using the gradients from backpropagation)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss_avg[-1] += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "    train_loss_avg[-1] /= num_batches\n",
        "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, num_epochs, train_loss_avg[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training ...\n",
            "Epoch [1 / 20] average reconstruction error: 1308049.387500\n",
            "Epoch [2 / 20] average reconstruction error: 1270866.902474\n",
            "Epoch [3 / 20] average reconstruction error: 1268790.706380\n",
            "Epoch [4 / 20] average reconstruction error: 1268026.495182\n",
            "Epoch [5 / 20] average reconstruction error: 1267539.707422\n",
            "Epoch [6 / 20] average reconstruction error: 1266973.376562\n",
            "Epoch [7 / 20] average reconstruction error: 1266684.756250\n",
            "Epoch [8 / 20] average reconstruction error: 1266218.359115\n",
            "Epoch [9 / 20] average reconstruction error: 1265846.184635\n",
            "Epoch [10 / 20] average reconstruction error: 1265593.286068\n",
            "Epoch [11 / 20] average reconstruction error: 1265187.449740\n",
            "Epoch [12 / 20] average reconstruction error: 1264644.451432\n",
            "Epoch [13 / 20] average reconstruction error: 1264152.101042\n",
            "Epoch [14 / 20] average reconstruction error: 1263749.008724\n",
            "Epoch [15 / 20] average reconstruction error: 1263321.304036\n",
            "Epoch [16 / 20] average reconstruction error: 1262577.776823\n",
            "Epoch [17 / 20] average reconstruction error: 1262245.637630\n",
            "Epoch [18 / 20] average reconstruction error: 1261649.718750\n",
            "Epoch [19 / 20] average reconstruction error: 1261021.131641\n",
            "Epoch [20 / 20] average reconstruction error: 1260262.573177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-24T23:03:14.747789Z",
          "iopub.status.busy": "2021-04-24T23:03:14.746833Z",
          "iopub.status.idle": "2021-04-24T23:03:42.576821Z",
          "shell.execute_reply": "2021-04-24T23:03:42.576005Z"
        },
        "papermill": {
          "duration": 27.874551,
          "end_time": "2021-04-24T23:03:42.576955",
          "exception": false,
          "start_time": "2021-04-24T23:03:14.702404",
          "status": "completed"
        },
        "tags": [],
        "id": "XyFrbi0Z8p7T",
        "outputId": "cce95c53-b04c-48c7-ddaa-cd2ef291bc59"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "zipObj = ZipFile('images.zip', 'w')\n",
        "for i in range(7000):\n",
        "    img = img_recon[i]\n",
        "    fp = 'img' + str(i) + '.jpg'\n",
        "    torchvision.utils.save_image(img, fp)\n",
        "    zipObj.write(fp)\n",
        "zipObj.close()\n",
        "!rm *.jpg\n",
        "print(\"done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}